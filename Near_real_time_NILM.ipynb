{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Near real-time NILM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1H8VxJI_vMPmqpBld69OOvnW4PkyGgI3g",
      "authorship_tag": "ABX9TyMGiFNR5DXhP66eW9hF4F9o"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lT8hTh98iQg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()\n",
        "os.chdir('/content/drive/MyDrive/REDD')"
      ],
      "metadata": {
        "id": "jwpuTObX_HSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data:"
      ],
      "metadata": {
        "id": "7iuwcitm_AM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "fr = pd.read_csv('channel_5.dat', header=None, sep=' ', engine='python', names=['a', 'b'])\n",
        "fr = fr.b\n",
        "ov = pd.read_csv('channel_3.dat', header=None, sep=' ', engine='python', names=['a', 'b'])\n",
        "ov = ov.b \n",
        "dw = pd.read_csv('channel_6.dat', header=None, sep=' ', engine='python', names=['a', 'b'])\n",
        "dw = dw.b\n",
        "kit = pd.read_csv('channel_8.dat', header=None, sep=' ', engine='python', names=['a', 'b'])\n",
        "kit = kit.b\n",
        "wd = pd.read_csv('channel_10.dat', header=None, sep=' ', engine='python', names=['a', 'b'])\n",
        "wd = wd.b\n",
        "mic = pd.read_csv('channel_11.dat', header=None, sep=' ', engine='python', names=['a', 'b'])\n",
        "mic = mic.b "
      ],
      "metadata": {
        "id": "Ulv_rnty_BcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create aggregate vector:"
      ],
      "metadata": {
        "id": "AbmV2EW0AQuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg = fr + ov + dw + kit + wd + mic \n",
        "# reshape \n",
        "agg = agg.to_numpy()\n",
        "X = agg.reshape((len(agg),1))"
      ],
      "metadata": {
        "id": "KsE4yTzUASki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label data:"
      ],
      "metadata": {
        "id": "2Gi_YdlMAbkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thr =50\n",
        "\n",
        "def labeling(data):\n",
        "    labels =[]\n",
        "    for i in range(0,len(data)):\n",
        "        if (data[i]>thr):\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(0)\n",
        "            \n",
        "    return np.array(labels) \n",
        "#%%\n",
        "y_fr = labeling (fr)\n",
        "y_fr = y_fr.reshape((len(y_fr),1))\n",
        "\n",
        "y_ov = labeling (ov)\n",
        "y_ov = y_ov.reshape((len(y_ov),1))\n",
        "\n",
        "y_dw = labeling (dw)\n",
        "y_dw = y_dw.reshape((len(y_dw),1))\n",
        "\n",
        "y_kit = labeling (kit)\n",
        "y_kit = y_kit.reshape((len(y_kit),1))\n",
        "\n",
        "y_wd = labeling (wd)\n",
        "y_wd = y_wd.reshape((len(y_wd),1))\n",
        "\n",
        "y_mic = labeling (mic)\n",
        "y_mic = y_mic.reshape((len(y_mic),1))"
      ],
      "metadata": {
        "id": "S4VMkgHXAdEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create label matrix:"
      ],
      "metadata": {
        "id": "N352VKm-Apum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.concatenate((y_fr,y_ov,y_dw,y_kit,y_wd,y_mic), axis=1)"
      ],
      "metadata": {
        "id": "3lGddAGhArNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "STATE ESTIMATION USING MULTILABEL CLASSIFICATION :\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cp_kbsb8EFte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into train/test sets:"
      ],
      "metadata": {
        "id": "34X-HHLdBFQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
        "                                                    test_size=0.2)"
      ],
      "metadata": {
        "id": "gpUPXg35BHlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions:"
      ],
      "metadata": {
        "id": "BDjMULkyBlau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model):\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def evaluate():\n",
        "    \n",
        "    y_pred_train = model.predict(X_train)\n",
        "    \n",
        "    micro_f1 = f1_score(y_train, y_pred_train>0.5, average='micro')\n",
        "    macro_f1 = f1_score(y_train, y_pred_train>0.5, average='macro') \n",
        "\n",
        "    print(\"training performance\")\n",
        "    print(\"micro-F1: %s\"%micro_f1)\n",
        "    print(\"macro-F1: %s\"%macro_f1)\n",
        "    # print(\"training error: %s\"%error_training)\n",
        "    print(\"\")\n",
        "\n",
        "    y_pred_test = model.predict(X_test)\n",
        "        \n",
        "    micro_f1 = f1_score(y_test, y_pred_test>0.5, average='micro')\n",
        "                         \n",
        "    macro_f1 = f1_score(y_test, y_pred_test>0.5, average='macro') \n",
        "    \n",
        "    print(\"testing performance\")\n",
        "    print(\"micro-F1: %s\"%micro_f1)\n",
        "    print(\"macro-F1: %s\"%macro_f1)\n",
        "    # print(\"training error: %s\"%error_training)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "_DKIrMOoBmnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate the classfier:"
      ],
      "metadata": {
        "id": "hz01WczHBuAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "train(model)\n",
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3-kQyrYBwQJ",
        "outputId": "10194edf-2925-455f-ad3b-4b7d81658a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training performance\n",
            "micro-F1: 0.9221090962849995\n",
            "macro-F1: 0.8566080314924207\n",
            "\n",
            "testing performance\n",
            "micro-F1: 0.9175432327030344\n",
            "macro-F1: 0.8253331774663012\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to improve performance by reducing variance in the aggregate signal:"
      ],
      "metadata": {
        "id": "GV0y1uvJCsPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" if the next agg sample doesn't vary by an amount greater\n",
        "than thr_p, then copy the current agg sample \"\"\" \n",
        "thr_p=10\n",
        "agg_stable = np.copy(agg)\n",
        " \n",
        "for i in range (0, len(agg)-1):\n",
        "    if (abs(agg_stable[i]-agg_stable[i+1])<thr_p):\n",
        "        agg_stable[i+1] = agg_stable[i]\n",
        "    else:\n",
        "         pass\n",
        "\n",
        "\"\"\" train with agg_stable \"\"\"\n",
        "X = agg_stable.reshape((len(agg_stable),1))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
        "                                                    test_size=0.2,random_state=1)\n",
        "\n",
        "train(model)\n",
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7n_IipeCy75",
        "outputId": "e836e43a-59d8-4d65-9df4-4a2a9c60c6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training performance\n",
            "micro-F1: 0.9250315782508082\n",
            "macro-F1: 0.8825350793100041\n",
            "\n",
            "testing performance\n",
            "micro-F1: 0.9188659432971648\n",
            "macro-F1: 0.854839093793715\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance improves slightly."
      ],
      "metadata": {
        "id": "CRjN6yRBC9Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Compute metrcis:\n",
        "\n"
      ],
      "metadata": {
        "id": "HJ3Atkg3DLmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "y_pred_test = model.predict(X_test)\n",
        "conf_mtrx = multilabel_confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "TP_fr = conf_mtrx[0][0][0]\n",
        "FN_fr = conf_mtrx[0][0][1]\n",
        "FP_fr = conf_mtrx[0][1][0]\n",
        "TN_fr = conf_mtrx[0][1][1]\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "# fr + ov + dw + kit + wd + mic\n",
        "\n",
        "fr_acc = accuracy_score(y_test[:,0],y_pred_test[:,0])\n",
        "ov_acc = accuracy_score(y_test[:,1],y_pred_test[:,1])\n",
        "dw_acc = accuracy_score(y_test[:,2],y_pred_test[:,2])\n",
        "kit_acc = accuracy_score(y_test[:,3],y_pred_test[:,3])\n",
        "wd_acc = accuracy_score(y_test[:,4],y_pred_test[:,4])\n",
        "mic_acc = accuracy_score(y_test[:,5],y_pred_test[:,5])\n",
        "\n",
        "average_acc = np.average((fr_acc,ov_acc,dw_acc,kit_acc,wd_acc,mic_acc))\n",
        "\n",
        "print(\"accuracies\\n\")\n",
        "\n",
        "print(\"fr:{}\".format(fr_acc))\n",
        "print(\"ov:{}\".format(ov_acc))\n",
        "print(\"dw:{}\".format(dw_acc))\n",
        "print(\"kit:{}\".format(kit_acc))\n",
        "print(\"wd:{}\".format(wd_acc))\n",
        "print(\"mic:{}\".format(mic_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckX0-M8iDAVh",
        "outputId": "6d157f62-4fbc-4117-e6ba-f60caad44b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracies\n",
            "\n",
            "fr:0.9840322840135143\n",
            "ov:0.9991486566203679\n",
            "dw:0.9857215637904221\n",
            "kit:0.9672199281385746\n",
            "wd:0.9963465973078779\n",
            "mic:0.9976068536493806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "POWER ESTIMATION USING MULTIOUTPUT REGRESSION :\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "CpvFF7OdDk5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regression model is train/tested with Xe =[X|Y] as input and\n",
        "Ye = [fr,ov,dw,kit,wd,mic] as outputs."
      ],
      "metadata": {
        "id": "iT9B62i3Etud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xe = np.concatenate((X,Y), axis=1)\n",
        "\n",
        "fr = fr.to_numpy().reshape(-1,1)\n",
        "ov = ov.to_numpy().reshape(-1,1)\n",
        "dw = dw.to_numpy().reshape(-1,1)\n",
        "kit = kit.to_numpy().reshape(-1,1)\n",
        "wd = wd.to_numpy().reshape(-1,1)\n",
        "mic = mic.to_numpy().reshape(-1,1)\n",
        "\n",
        "Ye = np.concatenate((fr,ov,dw,kit,wd,mic), axis=1)"
      ],
      "metadata": {
        "id": "Vsryej0eD4Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data:"
      ],
      "metadata": {
        "id": "VtfKX0c0FEck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xe_train, Xe_test, ye_train, ye_test = train_test_split(Xe, Ye,\n",
        "                                                    test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "RhDqyvKfFFXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/test the regression model:"
      ],
      "metadata": {
        "id": "2NGsF0KhFUb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "\"\"\" training and testing the energy estimator\"\"\"\n",
        "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(Xe_train\n",
        "                                                      , ye_train)\n",
        "\"\"\" predict on training data \"\"\"\n",
        "y_energy_pred_train = clf.predict(Xe_train)\n",
        "\n",
        "\"\"\" predict on testing data \"\"\"\n",
        "y_energy_pred_test = clf.predict(Xe_test)\n",
        "\"\"\" testing with the output of the classifier \"\"\"\n",
        "\n",
        "\"\"\" estimate and compare with ye_test. We first combine\n",
        "X_test with the predictions of the classifer (y_pred_test)\"\"\"\n",
        "cassifier_out = np.concatenate((X_test,y_pred_test), axis=1)\n",
        "\n",
        "\"\"\" use the regressor to predict the power sample for each\n",
        "load which is a reconstitution of the signal of each load\"\"\"\n",
        "ener_est = clf.predict(cassifier_out)\n"
      ],
      "metadata": {
        "id": "F0v3kk3nFHX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute metrics:"
      ],
      "metadata": {
        "id": "CKeeihbJGC9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" compute the relative energy estimation error for each laod \"\"\"\n",
        "error_fr = abs(np.sum(ye_test[:,0])- \n",
        "                   np.sum(ener_est[:,0]))/np.sum(ye_test[:,0])\n",
        "error_ov = abs(np.sum(ye_test[:,1])- \n",
        "                   np.sum(ener_est[:,1]))/np.sum(ye_test[:,0])\n",
        "error_dw = abs(np.sum(ye_test[:,2])- \n",
        "                   np.sum(ener_est[:,2]))/np.sum(ye_test[:,0])\n",
        "error_kit = abs(np.sum(ye_test[:,3])- \n",
        "                   np.sum(ener_est[:,3]))/np.sum(ye_test[:,0])\n",
        "error_wd = abs(np.sum(ye_test[:,4])- \n",
        "                   np.sum(ener_est[:,4]))/np.sum(ye_test[:,0])\n",
        "error_mic = abs(np.sum(ye_test[:,5])- \n",
        "                   np.sum(ener_est[:,5]))/np.sum(ye_test[:,0])\n",
        "\n",
        "average_error = np.average((error_fr,error_ov,error_dw,error_kit,error_wd,error_mic))\n",
        "print(\"average error :{}\".format(average_error))\n",
        "\n",
        "\"\"\" compute NRMSE \"\"\"\n",
        "# reconstruct the predicted aggregate \n",
        "agg_pred = np.sum(ener_est, axis=1)\n",
        "agg_pred = agg_pred.reshape(-1,1)\n",
        "\n",
        "RMSE = np.sqrt(\n",
        "    np.sum( np.square(agg_pred-X_test)\n",
        "        )/len(X_test))\n",
        "\n",
        "NRMSE = RMSE/np.mean(X_test)\n",
        "print(\"NRMSE :{}\".format(NRMSE))\n",
        "\n",
        "\"\"\" compute F1 scores for each load \"\"\"\n",
        "\n",
        "f1_score_ = f1_score(y_test, y_pred_test, average=None)\n",
        "print(\"f1_score: {} \".format(f1_score_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv4cJesbGEAn",
        "outputId": "a5c4d4d7-cdc7-4efa-99ed-5168bc8e8f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average error :0.015576831100549013\n",
            "NRMSE :0.006579379520490179\n",
            "f1_score: [0.96955988 0.84717208 0.77383733 0.85626947 0.78347239 0.8987234 ] \n"
          ]
        }
      ]
    }
  ]
}